---
title: "HDI analyses"
author: "Andrew Edwards"
output: pdf_document
fontsize: 12pt
date: "Last compiled on `r format(Sys.time(), '%d %B, %Y')`"
---

```{r, echo = FALSE, eval = FALSE}
rmarkdown::render("hdi-analysis.Rmd")
```

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "",
  fig.width = 10,
  fig.height = 8
)
```

```{r, packages}
library(dplyr)
library(devtools)
# library(pacea)
load_all()
```

Had originally (in 2022) tried making histograms as they avoid smoothing, but fiddly and best
not to, as don't demonstrate the issue as clearly as I'd thought, because of the
first bins in the tails not being the same height (because they are bins); see
hdi-notes.pdf.

## Recruitment from Pacific Hake assessment

Next figure shows the idea. Top panel is what we calculate from the MCMC
outputs (for many quantities), with 95\% credible intervals calculated as having
endpoints at the 2.5\% and 97.5\% percentiles.

Bottom panel is calculating the highest density interval (HDI), which is meant
to be the narrowest of all possible 95\% credible intervals. It is calculated
using `HDInterval::hdi()`.

```{r recvals}
# plot_recruitment() calls create_intervals() which acts on all mcmc results to
# get intervals for each year, and uses HDInterval::hdi(). So do one example
year <- 2021
one_year_mcmc <- pull(hake_recruitment_mcmc,
                      as.character(year))    # Full 8000 MCMC samples for that year
one_year_mcmc_sort <- sort(one_year_mcmc)    # Useful later
# TODO save that as a data object in the package, prob don't need sort

head(one_year_mcmc_sort)
tail(one_year_mcmc_sort)
summary(one_year_mcmc_sort)
```

NOTE: default for hdi method is now to use density not the moving window method.

```{r rec, fig.width = 8, fig.height= 8}
par(mfrow = c(2,1))
# Equal probability 95% with regions 
plot_density_fig1(year = 2021, type = "equal")
plot_density(year = 2021, type = "hdi")
```

Figure for supplementary material to explain the regions of equal-tail.

```{r fig.regions.equal.tail, fig.width = 8, fig.height= 5}
plot_density_fig_sup(year = 2021, type = "equal")
```

However, the bottom panel does not quite look right, as the horizontal line
should intersect with the blue/red boundary.

Started to:

- fix the right-hand side
- shade areas to show the problem

Realise I should look around for alternative R packages (else we'll end up
reinventing the wheel and coming up with new methods, although such results
would be of broader interest probably). First explain `HDInterval::hdi()`.

Try a combined plot with other colours
```{r rec5, fig.height = 10}
# plot_density(year = 2021, type = "both")  # not implemented yet
```


## Explicit calculations

So we have `one_year_mcmc_sort` which has length
`r length(one_year_mcmc_sort)`. Have one function to calculate everything:

```{r calc}
one_year_res <- calc_density(one_year_mcmc_sort)
one_year_res
one_year_res$intervals %>% a()
```

## How does `HDInterval::hdi()` work?

Dig into analyses to understand better. From `?HDInterval::hdi`:

`None of the above use interpolation: the values returned
correspond to specific values in the data object, and will be
conservative (ie, too wide rather than too narrow). Results thus
depend on the random draws, and will be unstable if few values are
provided. For a 95% HDI, 10,000 independent draws are recommended;
a smaller number will be adequate for a 80% HDI, many more for a
99% HDI.`

So there is no interpolation, values should be specific values in the data
object, which explains why the y values look different above. So, let's confirm that:

```{r rec2}
hdi_res <- HDInterval::hdi(one_year_mcmc)
hdi_res
sum(one_year_mcmc == hdi_res["lower"])   # = 1, so is an exact value.
sum(one_year_mcmc == hdi_res["upper"])   # = 1, as expected
```

So presumably `hdi()` just shifts the interval along to get to the shortest 95%
value. I'd thought (before carefully reading the help quoted above) that it does
do some interpolation. See exactly what it does. Best to look at
https://github.com/mikemeredith/HDInterval . Basically, all methods (based on
class of object) use `hdiVector()`, which is not exported, but is (from GitHub):

```{r hdiVector, eval=FALSE}
# This is the function to deal with a 'raw' vector
# Not exported.

# Returns NAs for non-numeric input or all-NA input

hdiVector <- function(object, credMass=0.95, ...) {
  result <- c(NA_real_, NA_real_)
  if(is.numeric(object)) {
    attributes(object) <- NULL
    x <- sort.int(object, method='quick')  # removes NA/NaN, but not Inf
    n <- length(x)
    if(n > 0) {
      # exclude <- ceiling(n * (1 - credMass)) # Not always the same as...
      exclude <- n - floor(n * credMass)       # Number of values to exclude
      low.poss <- x[1:exclude]             # Possible lower limits...
      upp.poss <- x[(n - exclude + 1):n]   # ... and corresponding upper limits
      best <- which.min(upp.poss - low.poss)      # Combination giving the narrowest interval
      if(length(best)) {
        result <- c(low.poss[best], upp.poss[best])
      } else {
        tmp <- range(x)
        if(length(tmp) == 2)
          result <- tmp
      }
    }
  }
  names(result) <- c("lower", "upper")
  return(result)
}
```

So, because the calculated interval is actually wider than the 'true' interval,
the interpolation in my figure should be tweaked. Doing that now in
`plot_density()`, with some thinking here:

```{r calcs}
range(diff(one_year_res$density$x))      # x values are equally spaced
res <- one_year_res$intervals

res$i_low_hdi
res$hdi_lower    # TODO make consistent

# Recruitment density values around HDI low:
one_year_res$density$x[(res$i_low_hdi - 1):(res$i_low_hdi + 1)]

# And the density value, which is from hdi(), and the
# values around it, which are from density calculation (I think this is the crux
# of the problem TODO and is why the lines don't match up in the figure).
res$y_low_hdi_interp
one_year_res$density$y[(res$i_low_hdi - 1):(res$i_low_hdi + 1)]

# Can see both are in interval that corresponds to [i_low_hdi, i_low_hdi+1), so original interpolation is
# correct in plot_density().

# Do the same for high:
res$i_high_hdi
res$hdi_upper    # TODO make consistent

# Recruitment density values around HDI high:
one_year_res$density$x[(res$i_high_hdi - 1):(res$i_high_hdi + 1)]

# And the density value, which is from hdi(), and the
# values around it, which are from density calculation (I think this is the crux
# of the problem TODO and is why the lines don't match up in the figure).
res$y_high_hdi_interp
one_year_res$density$y[(res$i_high_hdi - 1):(res$i_high_hdi + 1)]

# Can see both are in interval that corresponds to [i_high_hdi, i_high_hdi+1), so original interpolation is
# correct in plot_density().

# So again, both are indeed correctly in what corresponds to [i_high,
# i_high+1).
```



So the trouble is actually the HDI calculation NOT doing any interpolation, only
using the given values. But in the tail these are spread out. ALSO, `density()`
does equally spaced values, but the tail of the data is so spread out:
```{r tail}
tail(one_year_res$density$x)
tail(one_year_mcmc_sort)
```
so lots of values are wasted in the big gap at the end.
```{r rug}
plot(one_year_res$dens)
rug(one_year_mcmc_sort)
rug(one_year_res$density$x, side = 3)
# abline(v = rr$dens$x[rr$ints$i_high_hdi])
```

HERE - roughly what was doing, need to tidy things up.

```{r fripm}
rr <- calc_density(one_year_mcmc)
plot_density(one_year_mcmc) #, x_lim=c(0, 130))
abline(v = rr$dens$x[rr$intervals$i_high_hdi])
abline(v = rr$dens$x[rr$intervals$i_high_hdi-1], col="green")
abline(v = rr$dens$x[rr$intervals$i_low_hdi-1], col="green")
abline(v = rr$dens$x[rr$intervals$i_low_hdi], col="black")
```

How about doing the HDI on the density version (which is kind of what I assumed
it was doing anyway). Ideally like to figure out the problem with doing it on
the MCMC values, but that could be an option. Need to draw some figures.
Don't think I need `plot_recruitment_density()` in it's current state, as
simplified to `plot_density()`.

See if Marie knows more about `?density` kernels etc.


## Longer chain of 32000

```{r long}
year <- 2021
one_year_mcmc_32000 <- pull(hake_recruitment_mcmc_14_long,
                      as.character(year))    # Full 32000 MCMC samples for that year
one_year_mcmc_32000_sort <- sort(one_year_mcmc_32000)    # Useful later
# TODO save that as a data object in the package, prob don't need sort

head(one_year_mcmc_32000_sort)
tail(one_year_mcmc_32000_sort)
summary(one_year_mcmc_32000_sort)
```

So presumably the longer chain should not have some of the above issues:
```{r reclong, fig.height = 10}
par(mfrow = c(2,1))
plot_density(one_year_mcmc_32000, year = 2021, type = "equal")
plot_density(one_year_mcmc_32000, year = 2021, type = "hdi")
```

wtf it does. Maybe it's still too sparse in that area.
```{r calcs7}
sum(one_year_mcmc_32000 < 28 & one_year_mcmc_32000 > 23)
```

Do the calculations:
```{r calclong}
one_year_res_32000 <- calc_density(one_year_mcmc_32000_sort)
one_year_res_32000
one_year_res_32000$intervals %>% a()
```


```{r reclong2, fig.height = 10}
# par(mfrow = c(2,1))
# plot_density(one_year_mcmc_32000, year = 2021, type = "equal")
plot_density(one_year_mcmc_32000, x_lim = c(20, 30), year = 2021, type = "hdi")
rug(one_year_mcmc_32000)
rug(one_year_res_32000$density$x, side = 3)
```
Think that shouldn't happen and there's a bug somewhere. Try the other end:

```{r reclong88, fig.height = 10}
# par(mfrow = c(2,1))
# plot_density(one_year_mcmc_32000, year = 2021, type = "equal")
plot_density(one_year_mcmc_32000, x_lim = c(0, 5), year = 2021, type = "hdi")
rug(one_year_mcmc_32000)
rug(one_year_res_32000$density$x, side = 3)
```

Does feel like it would be better to do the calculation on the density? Because
it can use the pdf values (I think one of the functions can) and be closer to
the 'true' HDI. Seems clear that extra MCMC samples isn't helping, but I think
there's a bug somewhere.



## Other packages?

- From `?HDInterval::hdi`:

Kruschke, J. K. 2011. _Doing Bayesian data analysis: a tutorial
     with R and BUGS._ Elsevier, Amsterdam, section 3.3.5.

See below for notes baed on second edition.


Before writing new methods, see what else is available in R.

- Could calculate the pdf values based on the log(pdf)? Not sure.
- The longer answer here (scroll down) has some interesting ideas:
  https://stats.stackexchange.com/questions/381520/how-can-i-estimate-the-highest-posterior-density-interval-from-a-set-of-x-y-valu

- `bayestestR::hdi()` -- also has `eti()` (equal-tailed intervals
  function). Gives credit to HDInterval, so maybe uses that anyway. Start
  digging:



## Further thoughts

- Equal-tailed intervals do not change under a nonlinear transformation of the values, but
HDI ones will. Kruschke 2015 p343 explains it nicely, and points out that
usually we're working on the correct scale anyway (do we care about numbers of
fish, or log of the numbers of fish?).

- There's also talk of 89\% intervals (something to do with 89 being the highest
  prime number below 95; see `bayestestR::hdi()`.

- Kruschke 2015 (saved pdf), searching whole book for `hdi`. Comes up a lot as
  in lots of figures, and a bit repetitive (think book is designed to read
  chapters somewhat independently), but haven't found clear explanation of how
  calcualted on MCMC samples. Lots is to do with if HDI overlaps some value of
  interest (e.g. 0, kind of null hypothesis).

 - p87 on credible intervals. Explains why sticking
  with HDI (great, we can just reference), and also says posterior highest
  density intervals (priors can have them too). Then see p342, which has the
  usual figure (as does something earlier). p184 talks about
  ESS and HDI, recommend 10,000 (based on experience). We could test. Or even
  simulate from a lognormal with a long tail. But whole book doesn't really seem
  to dig into how to calculate the HDI from empiricial data -- though lots of
  examples so it must do somewhere. Does say (p117):
  "You will notice that for sparse grids on $\theta$, it is usually impossible to span grid values that
exactly achieve the desired HDI, and therefore, the bars that minimally exceed the HDI
mass are displayed. " Sounds like same method as HDInterval; could dig into
  their code which is given somewhere. p181: mentions different chains giving
  slightly different HDI. for infinite chains, all converge,

  - p294 uses `HDIofICDF()` that comes with book (not sure what ICDF stands for
    yet). See also mention on p346.

  - p330 different widths based on simulations and sample size: " Suppose we contemplate a survey that polls 500
people. By simulating the experiment over and over, using the hypothesized random
$\theta \simeq 0.60$ and N = 500, we can generate simulated data, and then derive a Bayesian
posterior distribution for every set of simulated data. For every posterior distribution,
we determine some measure of accuracy, such as the width of the 95\% HDI. From
many simulated experiments, we get a sampling distribution of HDI widths. From the
sampling distribution of HDI widths, we can decide whether N = 500 typically yields
high enough accuracy for our purposes.If not, we repeat the simulation with a larger N.
Once we know how big N needs to be to get the accuracy we seek, we can decide
whether or not it is feasible to conduct such a study."

  - p365: "There are other ways to express mathematically the goal of precision in estimation. For
example, another way of using HDIs was described by Joseph, Wolfson, and du Berger
(1995a, 1995b). They considered an “average length criterion,” which requires that
the average HDI width, across repeated simulated data, does not exceed some maximal
value L. There is no explicit mention of power, i.e., the probability of achieving the
goal, because the sample size is chosen so that the goal is definitely achieved. The goal
itself is probabilistic, however, because it regards an average:  While some data sets will
have HDI width less than L, many other data sets will not have an HDI width greater
than L." Not quite sure of the point here.

  - p369 has some code, think still really just need `HDIofICDF()` (haven't come
    across it yet) in `DBDA2E-utilities.R`. Ooh, there's also `HDIofMCMC()`
    first seen on p373 -- described on p725 (see below).

  - p371, simulation study for sample size needed for HDI to have a given
    maximal width, for coin flipping.

  - p386 Figure is what we were going to do (just bottom panels), given a long
    chain. Maybe don't need the width one, ooh, just the sample size one but for
    independently run MCMC chains (so some might stop earlier).
	Dashed lines are the ROPE, which I don't understand yet, don't think we need.

  - p725 - rising tide description of HDI, with poles. And code, which is also
    given and adapted in
    https://bookdown.org/content/3686/tools-in-the-trunk.html#functions-for-computing-highest-density-intervals
    who says:
	Just in case you’re curious, Kruschke’s `HDIofMCMC()` function returns the same information as `tidybayes::hdi()`. Let’s confirm.

  - Looks like Kruschke's code does the same as `HDInterval::hdi()`.


## Proposition -- can we do density first and then calculate HDI? Yes, this can
## be done in `HDIinterval::hdi()`

This is what I kind of assumed was being done. Not ideal as is approximating the
density of the MCMC sample. But given the issues with our HDI not really being
the HDI (even for 8,000 samples; admittedly the recommendation is 10,000).

# Wednesday at UBC

Increasing $n$ for the density calculations does not change the wiggliness of
the density plot, but will increase resolutions and presumably generate a more
accurate HDI. We were still getting the original issue with higher $n$.

## Original 2021 data set, with higher $n$ to then calculate the density

```{r highn}
head(one_year_mcmc_sort)
tail(one_year_mcmc_sort)

res_high_n <- calc_density(one_year_mcmc_sort, n = 1e5)
res_high_n$intervals %>% a()
plot_density(one_year_mcmc_sort,
             dens_intervals = res_high_n)
```

## Longer 32,000 chain

TODO check figure still looks the same as previous, I had `res_high_n` instead
of `res_high_n_32000` here

```{r highnlong}
head(one_year_mcmc_32000_sort)
tail(one_year_mcmc_32000_sort)

res_high_n_32000 <- calc_density(one_year_mcmc_32000_sort, n = 1e5)
res_high_n_32000$intervals %>% a()
plot_density(one_year_mcmc_32000_sort,
             dens_intervals = res_high_n_32000)
```


## Relative spawning biomass

Using 2024 base case model relative biomass MCMC results, saved in package.

```{r relbiomass}
current_relative_biomass_mcmc <- pull(hake_relative_biomass_mcmc,
                                       as.character(2024))    # Full 8000 MCMC samples for that year
current_relative_biomass_mcmc_sort <- sort(current_relative_biomass_mcmc)

head(current_relative_biomass_mcmc_sort)
tail(current_relative_biomass_mcmc_sort)

res_relative_biomass <- calc_density(current_relative_biomass_mcmc_sort,
                                     n = 1e5)
res_relative_biomass$intervals %>% a()
plot_density(current_relative_biomass_mcmc_sort,
             dens_intervals = res_relative_biomass,
             x_lim = c(0, 4),
             x_lab = "Relative spawning biomass in 2024")
```
